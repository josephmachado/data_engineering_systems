{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "675ddeef-f650-4bc3-b09c-f70b93df2b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b52292f501e7:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x722214591810>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark\n",
    "# our spark session is available as spark variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893ef439-6877-4cd0-826d-c8aa49f8daf7",
   "metadata": {},
   "source": [
    "# DDL tables\n",
    "\n",
    "Our catalog is named prod."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be46061-f82f-4701-9278-da5c6d7347be",
   "metadata": {},
   "source": [
    "## Upstream tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04d53262-eebc-4f7e-875b-27d2755a1943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bdd8dba6-b604-49e2-b9c1-95872918d582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS prod.db.orders\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS prod.db.orders\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS prod.db.orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0d0fba84-5e3f-4d6f-b2ce-b4cafe7cd857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Table DDL for OLTP tables\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS prod.db.customer (\n",
    "    customer_id INT,\n",
    "    email STRING,\n",
    "    first_name STRING,\n",
    "    last_name STRING,\n",
    "    phone STRING,\n",
    "    status STRING,\n",
    "    is_verified BOOLEAN,\n",
    "    registration_date TIMESTAMP,\n",
    "    last_login_date TIMESTAMP,\n",
    "    datetime_created TIMESTAMP,\n",
    "    datetime_updated TIMESTAMP\n",
    ") USING iceberg\n",
    "TBLPROPERTIES (\n",
    "    'format-version' = '2'\n",
    ")\"\"\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "-- Profile table (child to customer - many profiles per customer)\n",
    "CREATE TABLE IF NOT EXISTS prod.db.profile (\n",
    "    profile_id INT,\n",
    "    customer_id INT,\n",
    "    profile_name STRING,\n",
    "    profile_type STRING,\n",
    "    address_line1 STRING,\n",
    "    address_line2 STRING,\n",
    "    city STRING,\n",
    "    state STRING,\n",
    "    postal_code STRING,\n",
    "    country STRING,\n",
    "    is_default BOOLEAN,\n",
    "    datetime_created TIMESTAMP,\n",
    "    datetime_updated TIMESTAMP\n",
    ") USING iceberg\n",
    "TBLPROPERTIES (\n",
    "    'format-version' = '2'\n",
    ");\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "-- Orders table (references profile)\n",
    "CREATE TABLE IF NOT EXISTS prod.db.orders (\n",
    "    order_id INT,\n",
    "    customer_id INT,\n",
    "    order_date TIMESTAMP,\n",
    "    order_status STRING,\n",
    "    payment_method STRING,\n",
    "    payment_status STRING,\n",
    "    subtotal DECIMAL(10, 2),\n",
    "    tax_amount DECIMAL(10, 2),\n",
    "    shipping_amount DECIMAL(10, 2),\n",
    "    discount_amount DECIMAL(10, 2),\n",
    "    total_amount DECIMAL(10, 2),\n",
    "    currency STRING,\n",
    "    shipping_method STRING,\n",
    "    tracking_number STRING,\n",
    "    notes STRING,\n",
    "    datetime_created TIMESTAMP\n",
    ") USING iceberg\n",
    "TBLPROPERTIES (\n",
    "    'format-version' = '2'\n",
    ");\n",
    "\"\"\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "29d766d2-489b-4045-a4fe-94704428cc40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert some fake data for the OLTP tables\n",
    "spark.sql(\"\"\"\n",
    "-- Insert sample customers\n",
    "INSERT INTO prod.db.customer VALUES\n",
    "  (1, 'john.doe@example.com', 'John', 'Doe', '555-123-4567', 'active', true, TIMESTAMP '2023-01-15 08:30:00', TIMESTAMP '2023-03-20 14:22:15', TIMESTAMP '2023-01-15 08:30:00', TIMESTAMP '2023-03-20 14:22:15'),\n",
    "  (2, 'jane.smith@example.com', 'Jane', 'Smith', '555-987-6543', 'active', true, TIMESTAMP '2023-02-05 12:45:00', TIMESTAMP '2023-03-18 09:10:30', TIMESTAMP '2023-02-05 12:45:00', TIMESTAMP '2023-03-18 09:10:30'),\n",
    "  (3, 'robert.brown@example.com', 'Robert', 'Brown', '555-456-7890', 'inactive', false, TIMESTAMP '2023-01-25 15:20:00', TIMESTAMP '2023-02-10 11:05:45', TIMESTAMP '2023-01-25 15:20:00', TIMESTAMP '2023-02-10 11:05:45');\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "-- Insert sample profiles\n",
    "INSERT INTO prod.db.profile VALUES\n",
    "  (101, 1, 'Home Address', 'home', '123 Main St', 'Apt 4B', 'New York', 'NY', '10001', 'US', true, TIMESTAMP '2023-01-15 08:35:00', TIMESTAMP '2023-01-15 08:35:00'),\n",
    "  (102, 1, 'Work Address', 'work', '456 Business Ave', 'Suite 200', 'New York', 'NY', '10002', 'US', false, TIMESTAMP '2023-02-10 09:20:00', TIMESTAMP '2023-02-10 09:20:00'),\n",
    "  (103, 2, 'Shipping Address', 'shipping', '789 Residential Blvd', NULL, 'Los Angeles', 'CA', '90001', 'US', true, TIMESTAMP '2023-02-05 12:50:00', TIMESTAMP '2023-02-05 12:50:00'),\n",
    "  (104, 2, 'Billing Address', 'billing', '101 Finance St', '15th Floor', 'Los Angeles', 'CA', '90002', 'US', false, TIMESTAMP '2023-02-05 12:55:00', TIMESTAMP '2023-02-05 12:55:00'),\n",
    "  (105, 3, 'Home Address', 'home', '202 Cedar Lane', NULL, 'Chicago', 'IL', '60601', 'US', true, TIMESTAMP '2023-01-25 15:25:00', TIMESTAMP '2023-01-25 15:25:00');\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "-- Insert sample orders\n",
    "INSERT INTO prod.db.orders VALUES\n",
    "  (1001, 1, TIMESTAMP '2023-02-01 10:15:00', 'delivered', 'credit_card', 'paid', 89.99, 7.20, 5.99, 0.00, 103.18, 'USD', 'standard', 'TRK123456789', NULL, TIMESTAMP '2023-02-01 10:15:00'),\n",
    "  (1002, 1, TIMESTAMP '2023-03-10 14:30:00', 'shipped', 'paypal', 'paid', 45.50, 3.64, 5.99, 5.00, 50.13, 'USD', 'express', 'TRK987654321', NULL, TIMESTAMP '2023-03-10 14:30:00'),\n",
    "  (1003, 2, TIMESTAMP '2023-02-20 09:45:00', 'delivered', 'credit_card', 'paid', 129.95, 10.40, 9.99, 15.00, 135.34, 'USD', 'standard', 'TRK456789123', 'Please leave at front door', TIMESTAMP '2023-02-20 09:45:00'),\n",
    "  (1004, 2, TIMESTAMP '2023-03-15 16:20:00', 'processing', 'apple_pay', 'paid', 75.25, 6.02, 5.99, 0.00, 87.26, 'USD', 'standard', NULL, NULL, TIMESTAMP '2023-03-15 16:20:00'),\n",
    "  (1005, 3, TIMESTAMP '2023-02-05 11:10:00', 'cancelled', 'credit_card', 'refunded', 199.99, 16.00, 12.99, 20.00, 208.98, 'USD', 'overnight', NULL, 'Customer requested cancellation', TIMESTAMP '2023-02-05 11:10:00');\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16273753-d688-4fc3-9362-34b7f1fab1ea",
   "metadata": {},
   "source": [
    "## Dimensional tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8ff638ac-efcb-497f-a087-4531bd0a39b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS prod.db.dim_customer\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS prod.db.fct_orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4dd4c209-26fe-44e9-9d29-d1f2ecaeb7a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "-- Customer Dimension Table with profiles as array of structs\n",
    "CREATE TABLE IF NOT EXISTS prod.db.dim_customer (\n",
    "    customer_id INT,\n",
    "    email STRING,\n",
    "    first_name STRING,\n",
    "    last_name STRING,\n",
    "    phone STRING,\n",
    "    status STRING,\n",
    "    is_verified BOOLEAN,\n",
    "    registration_date TIMESTAMP,\n",
    "    last_login_date TIMESTAMP,\n",
    "    \n",
    "    -- Profiles as array of structs\n",
    "    profiles ARRAY<STRUCT<\n",
    "        profile_id: INT,\n",
    "        profile_name: STRING,\n",
    "        profile_type: STRING,\n",
    "        address_line1: STRING,\n",
    "        address_line2: STRING,\n",
    "        city: STRING,\n",
    "        state: STRING,\n",
    "        postal_code: STRING,\n",
    "        country: STRING,\n",
    "        is_default: BOOLEAN,\n",
    "        datetime_created: TIMESTAMP,\n",
    "        datetime_updated: TIMESTAMP\n",
    "    >>,\n",
    "    \n",
    "    datetime_created TIMESTAMP,\n",
    "    datetime_updated TIMESTAMP\n",
    ") USING iceberg\n",
    "TBLPROPERTIES (\n",
    "    'format-version' = '2'\n",
    ");\"\"\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "-- Orders Fact Table\n",
    "CREATE TABLE IF NOT EXISTS prod.db.fct_orders (\n",
    "    order_id INT,\n",
    "    \n",
    "    -- Foreign keys\n",
    "    customer_id INT,\n",
    "    \n",
    "    -- Time dimensions\n",
    "    order_date TIMESTAMP,\n",
    "    \n",
    "    -- Order attributes\n",
    "    order_status STRING,\n",
    "    payment_method STRING,\n",
    "    payment_status STRING,\n",
    "    \n",
    "    -- Order metrics\n",
    "    subtotal DECIMAL(10, 2),\n",
    "    tax_amount DECIMAL(10, 2),\n",
    "    shipping_amount DECIMAL(10, 2),\n",
    "    discount_amount DECIMAL(10, 2),\n",
    "    total_amount DECIMAL(10, 2),\n",
    "    \n",
    "    -- Order details\n",
    "    currency STRING,\n",
    "    shipping_method STRING,\n",
    "    tracking_number STRING,\n",
    "    notes STRING\n",
    "\n",
    ") USING iceberg\n",
    "TBLPROPERTIES (\n",
    "    'format-version' = '2'\n",
    ");\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258a38f3-836d-4d4d-b181-94f627019c92",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Gold tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514b0d2b-1644-407d-a32d-1984784e12c2",
   "metadata": {},
   "source": [
    "### OBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "615cdf88-f013-492b-8874-f51265c683b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS prod.db.obt_orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d1ab320c-dc83-4c33-965f-71d4d6cd81ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Order Business Table (OBT) - Denormalized view of orders with customer information\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS prod.db.obt_orders (\n",
    "    -- Order primary key\n",
    "    order_id INT,\n",
    "    \n",
    "    -- Order date and time attributes\n",
    "    order_date TIMESTAMP,\n",
    "    order_year INT,\n",
    "    order_month INT,\n",
    "    order_day INT,\n",
    "    order_dayofweek INT,\n",
    "    \n",
    "    -- Order attributes\n",
    "    order_status STRING,\n",
    "    payment_method STRING,\n",
    "    payment_status STRING,\n",
    "    shipping_method STRING,\n",
    "    tracking_number STRING,\n",
    "    notes STRING,\n",
    "    \n",
    "    -- Order metrics\n",
    "    subtotal DECIMAL(10, 2),\n",
    "    tax_amount DECIMAL(10, 2),\n",
    "    shipping_amount DECIMAL(10, 2),\n",
    "    discount_amount DECIMAL(10, 2),\n",
    "    total_amount DECIMAL(10, 2),\n",
    "    \n",
    "    -- Customer information\n",
    "    \n",
    "    customer ARRAY<STRUCT<\n",
    "    customer_id: INT,\n",
    "    email: STRING,\n",
    "    first_name: STRING,\n",
    "    last_name: STRING,\n",
    "    phone: STRING,\n",
    "    customer_status: STRING,\n",
    "    is_verified: BOOLEAN,\n",
    "    registration_date: TIMESTAMP,\n",
    "    last_login_date: TIMESTAMP,\n",
    "    -- Profiles as array of structs\n",
    "    profiles ARRAY<STRUCT<\n",
    "        profile_id: INT,\n",
    "        profile_name: STRING,\n",
    "        profile_type: STRING,\n",
    "        address_line1: STRING,\n",
    "        address_line2: STRING,\n",
    "        city: STRING,\n",
    "        state: STRING,\n",
    "        postal_code: STRING,\n",
    "        country: STRING,\n",
    "        is_default: BOOLEAN,\n",
    "        datetime_created: TIMESTAMP,\n",
    "        datetime_updated: TIMESTAMP\n",
    "    >>\n",
    "    >>,\n",
    "    \n",
    "    -- ETL metadata\n",
    "    source_system STRING,\n",
    "    etl_batch_id STRING,\n",
    "    etl_inserted TIMESTAMP,\n",
    "    etl_updated TIMESTAMP\n",
    ") USING iceberg\n",
    "TBLPROPERTIES (\n",
    "    'format-version' = '2'\n",
    ");\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae879ba-59ab-4fcf-a46a-e165e3b09efa",
   "metadata": {},
   "source": [
    "# Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ab7fe3-389f-4a48-b0ed-6b398b10f928",
   "metadata": {},
   "source": [
    "## Schema evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b30e2e30-e592-419d-b37d-2104489ddb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = spark.table(\"prod.db.customer\")\n",
    "profile_df = spark.table(\"prod.db.profile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bff057be-90d0-423d-a0f5-a5aad6f51e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "be768331-3718-4306-87c7-2fab4b5bb48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['profile_id',\n",
       " 'customer_id',\n",
       " 'profile_name',\n",
       " 'profile_type',\n",
       " 'address_line1',\n",
       " 'address_line2',\n",
       " 'city',\n",
       " 'state',\n",
       " 'postal_code',\n",
       " 'country',\n",
       " 'is_default',\n",
       " 'datetime_created',\n",
       " 'datetime_updated']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ffa4d313-25f8-48cc-994b-7c6024f32ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['profile_id',\n",
       " 'profile_name',\n",
       " 'profile_type',\n",
       " 'address_line1',\n",
       " 'address_line2',\n",
       " 'city',\n",
       " 'state',\n",
       " 'postal_code',\n",
       " 'country',\n",
       " 'is_default',\n",
       " 'datetime_created',\n",
       " 'datetime_updated']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_columns = [c for c in profile_df.columns if 'customer_id' not in c]\n",
    "new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ff2f0d33-df95-4e44-b8c4-4d0364113daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['profile_id',\n",
       " 'profile_type',\n",
       " 'address_line2',\n",
       " 'state',\n",
       " 'country',\n",
       " 'datetime_created']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_cols = new_columns[0:11:2]\n",
    "existing_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cf119244-e45f-4a2e-9af0-ae5baad30a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['postal_code',\n",
       " 'profile_name',\n",
       " 'city',\n",
       " 'datetime_updated',\n",
       " 'is_default',\n",
       " 'address_line1']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_add = list(set(new_columns) - set(existing_cols))\n",
    "cols_to_add # alter table; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ccc179bc-efce-4644-ab98-2ca2cd69c898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['profile_id',\n",
       " 'profile_type',\n",
       " 'address_line2',\n",
       " 'state',\n",
       " 'country',\n",
       " 'datetime_created',\n",
       " 'postal_code',\n",
       " 'profile_name',\n",
       " 'city',\n",
       " 'datetime_updated',\n",
       " 'is_default',\n",
       " 'address_line1']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_cols_order = existing_cols + cols_to_add\n",
    "new_cols_order # if existing cols are removed; they will be null going forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "40b5ae47-5451-424a-b50e-27e537be9ed6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|customer_id|profile_info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "+-----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1          |[{123 Main St, Apt 4B, New York, US, 2023-01-15 08:35:00, 2023-01-15 08:35:00, true, 10001, 101, Home Address, home, NY}, {456 Business Ave, Suite 200, New York, US, 2023-02-10 09:20:00, 2023-02-10 09:20:00, false, 10002, 102, Work Address, work, NY}, {123 Main St, Apt 4B, New York, US, 2023-01-15 08:35:00, 2023-01-15 08:35:00, true, 10001, 101, Home Address, home, NY}, {456 Business Ave, Suite 200, New York, US, 2023-02-10 09:20:00, 2023-02-10 09:20:00, false, 10002, 102, Work Address, work, NY}]                                                    |\n",
      "|2          |[{789 Residential Blvd, NULL, Los Angeles, US, 2023-02-05 12:50:00, 2023-02-05 12:50:00, true, 90001, 103, Shipping Address, shipping, CA}, {101 Finance St, 15th Floor, Los Angeles, US, 2023-02-05 12:55:00, 2023-02-05 12:55:00, false, 90002, 104, Billing Address, billing, CA}, {789 Residential Blvd, NULL, Los Angeles, US, 2023-02-05 12:50:00, 2023-02-05 12:50:00, true, 90001, 103, Shipping Address, shipping, CA}, {101 Finance St, 15th Floor, Los Angeles, US, 2023-02-05 12:55:00, 2023-02-05 12:55:00, false, 90002, 104, Billing Address, billing, CA}]|\n",
      "|3          |[{202 Cedar Lane, NULL, Chicago, US, 2023-01-25 15:25:00, 2023-01-25 15:25:00, true, 60601, 105, Home Address, home, IL}, {202 Cedar Lane, NULL, Chicago, US, 2023-01-25 15:25:00, 2023-01-25 15:25:00, true, 60601, 105, Home Address, home, IL}]                                                                                                                                                                                                                                                                                                                        |\n",
      "+-----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profile_df.groupBy(profile_df.customer_id).agg(\n",
    "    F.collect_list(\n",
    "        F.struct(sorted([c for c in profile_df.columns if 'customer_id' not in c]))\n",
    "    ).alias(\"profile_info\")\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "161cb711-4eaf-48bf-a846-353c477cc9e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['profile_id',\n",
       " 'profile_name',\n",
       " 'profile_type',\n",
       " 'address_line1',\n",
       " 'address_line2',\n",
       " 'city',\n",
       " 'state',\n",
       " 'postal_code',\n",
       " 'country',\n",
       " 'is_default',\n",
       " 'datetime_created',\n",
       " 'datetime_updated']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_customer = spark.table(\"prod.db.dim_customer\")\n",
    "[c.name for c in dim_customer.select(\"profiles\").schema.fields[0].dataType.elementType.fields] # get existing schema\n",
    "#[elt for elt in [r.dataType for r in dim_customer.select(\"profiles\").schema]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8bc2f651-0f3f-4c22-b957-082c2d001be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_struct_cols_to_add(new_col_list, old_col_list):\n",
    "    return sorted(list(set(new_col_list) - set(old_col_list)))\n",
    "\n",
    "last_col = old_col_list[-1]\n",
    "for new_col in get_struct_cols_to_add(new_col_list, old_col_list):\n",
    "    ddl = f\"ALTER TABLE prod.db.sample ADD COLUMN nested.new_column bigint FIRST\"\n",
    "    spark.sql(ddl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcdda088-ba0e-4e72-bb88-fdf6d1e81919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('profile_id', 'integer'),\n",
       " ('profile_name', 'string'),\n",
       " ('profile_type', 'string'),\n",
       " ('address_line1', 'string'),\n",
       " ('address_line2', 'string'),\n",
       " ('city', 'string'),\n",
       " ('state', 'string'),\n",
       " ('postal_code', 'string'),\n",
       " ('country', 'string'),\n",
       " ('is_default', 'boolean'),\n",
       " ('datetime_created', 'timestamp'),\n",
       " ('datetime_updated', 'timestamp')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(c.jsonValue().get('name'), c.jsonValue().get('type')) for c in spark.table(\"prod.db.dim_customer\").select(\"profiles\").schema.fields[0].dataType.elementType.fields] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4744ebd8-c4e0-4e4f-abca-75b37572408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "class DimCustomer:\n",
    "    def extract_upstream(self, start_ts, end_ts):\n",
    "        print(\"Starting EXRACT...\")\n",
    "        customer_df = spark.table(\"prod.db.customer\").where(F.col(\"datetime_updated\").between(start_ts, end_ts))\n",
    "        profile_df = spark.table(\"prod.db.profile\").where(F.col(\"datetime_updated\").between(start_ts, end_ts))\n",
    "        return {\n",
    "            \"customer\": customer_df,\n",
    "            \"profile\": profile_df\n",
    "        }\n",
    "    \n",
    "    def transform(self, input_dfs):\n",
    "        print(\"Starting TRANSFORM...\")\n",
    "        customer_df = input_dfs.get(\"customer\")\n",
    "        profile_df = input_dfs.get(\"profile\")\n",
    "        profile_struct_columns = [c for c in profile_df.columns if 'customer_id' not in c]\n",
    "\n",
    "        customer_cols = customer_df.columns\n",
    "\n",
    "        grouped_profile_id = profile_df.groupBy(\n",
    "            F.col(\"customer_id\")\n",
    "        ).agg(\n",
    "            F.collect_list(\n",
    "                F.struct(profile_struct_columns)\n",
    "            )\n",
    "        .alias(\"profiles\"))\n",
    "        \n",
    "        return customer_df.join(\n",
    "            grouped_profile_id,\n",
    "            on=\"customer_id\"\n",
    "        ).select(\n",
    "            *customer_cols\n",
    "            ,grouped_profile_id['profiles']\n",
    "        )\n",
    "    \n",
    "    def load(self, transformed_df):\n",
    "        print(\"Starting LOAD...\")\n",
    "        existing_struct_cols = [c.name for c in spark.table(\"prod.db.dim_customer\").select(\"profiles\").schema.fields[0].dataType.elementType.fields] # get existing schema\n",
    "        #new_struct_cols = [c.name for c in transformed_df.select(\"profiles\").schema.fields[0].dataType.elementType.fields]\n",
    "        new_col_data_types = [(c.jsonValue().get('name'), c.jsonValue().get('type')) for c in transformed_df.select(\"profiles\").schema.fields[0].dataType.elementType.fields] \n",
    "\n",
    "        #new_cols =  sorted(list(set(new_struct_cols) - set(existing_struct_cols)))\n",
    "        \n",
    "        last_col = existing_struct_cols[-1]\n",
    "        for new_col, data_type in new_col_data_types:\n",
    "            if new_col not in existing_struct_cols:\n",
    "                print(f'Updating dim_customer profiles schema...adding {new_col}')\n",
    "                ddl = f\"ALTER TABLE prod.db.dim_customer ADD COLUMN profiles.element.{new_col} {data_type} AFTER {last_col}\"\n",
    "                spark.sql(ddl)\n",
    "                last_col = new_col\n",
    "\n",
    "        print(\"Loading in data...\")\n",
    "        transformed_df.writeTo(\"prod.db.dim_customer\").overwritePartitions()\n",
    "        \n",
    "    \n",
    "    def run(self, start_ts, end_ts):\n",
    "        print(\"Starting RUN...\")\n",
    "        # Log: run_time, start_ts, end_ts, START state\n",
    "        self.load(self.transform(self.extract_upstream(start_ts, end_ts)))\n",
    "        # Log: run_time, start_ts, end_ts, END state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6726dc9c-df5c-437f-81bd-07dbb60a9937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RUN...\n",
      "Starting EXRACT...\n",
      "Starting TRANSFORM...\n",
      "Starting LOAD...\n",
      "Loading in data...\n"
     ]
    }
   ],
   "source": [
    "dim_customer = DimCustomer()\n",
    "dim_customer.run('2022-01-01', '2024-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bb08392e-c543-4421-87d7-d2802c21b4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------+---------+------------+--------+-----------+-------------------+-------------------+--------------------+-------------------+-------------------+\n",
      "|customer_id|               email|first_name|last_name|       phone|  status|is_verified|  registration_date|    last_login_date|            profiles|   datetime_created|   datetime_updated|\n",
      "+-----------+--------------------+----------+---------+------------+--------+-----------+-------------------+-------------------+--------------------+-------------------+-------------------+\n",
      "|          1|john.doe@example.com|      John|      Doe|555-123-4567|  active|       true|2023-01-15 08:30:00|2023-03-20 14:22:15|[{101, Home Addre...|2023-01-15 08:30:00|2023-03-20 14:22:15|\n",
      "|          1|john.doe@example.com|      John|      Doe|555-123-4567|  active|       true|2023-01-15 08:30:00|2023-03-20 14:22:15|[{101, Home Addre...|2023-01-15 08:30:00|2023-03-20 14:22:15|\n",
      "|          2|jane.smith@exampl...|      Jane|    Smith|555-987-6543|  active|       true|2023-02-05 12:45:00|2023-03-18 09:10:30|[{103, Shipping A...|2023-02-05 12:45:00|2023-03-18 09:10:30|\n",
      "|          2|jane.smith@exampl...|      Jane|    Smith|555-987-6543|  active|       true|2023-02-05 12:45:00|2023-03-18 09:10:30|[{103, Shipping A...|2023-02-05 12:45:00|2023-03-18 09:10:30|\n",
      "|          3|robert.brown@exam...|    Robert|    Brown|555-456-7890|inactive|      false|2023-01-25 15:20:00|2023-02-10 11:05:45|[{105, Home Addre...|2023-01-25 15:20:00|2023-02-10 11:05:45|\n",
      "|          3|robert.brown@exam...|    Robert|    Brown|555-456-7890|inactive|      false|2023-01-25 15:20:00|2023-02-10 11:05:45|[{105, Home Addre...|2023-01-25 15:20:00|2023-02-10 11:05:45|\n",
      "+-----------+--------------------+----------+---------+------------+--------+-----------+-------------------+-------------------+--------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from prod.db.dim_customer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de00360-d07d-4c39-8e24-179324596f5f",
   "metadata": {},
   "source": [
    "## Pipelines time chunked; single-run catchup; backfillable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "96b8c72a-3260-417c-bb3d-9b9a43c86a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The inputs are usually provided by the orchestrator\n",
    "\n",
    "# The logic depends on implementation; and some orchestrators provide backfill, but not catchup\n",
    "\n",
    "# start and end times\n",
    "\n",
    "# log table: catchup flag -> check log table for last end ts and run from them\n",
    "# Note: Airflow's catchup means it will run the pipeline from pipeline start date (one run per schedule till today)\n",
    "# backfill: run one time chunk at a time; favored over catchup for its ability to use prior chunks data for \n",
    "# current chunks computation\n",
    "\n",
    "# overwrites by partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60a4c3b-a590-4720-b384-b9d043e2a541",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
