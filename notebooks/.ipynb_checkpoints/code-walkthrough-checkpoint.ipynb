{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "675ddeef-f650-4bc3-b09c-f70b93df2b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b52292f501e7:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f7dea1a1810>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark\n",
    "# our spark session is available as spark variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893ef439-6877-4cd0-826d-c8aa49f8daf7",
   "metadata": {},
   "source": [
    "# DDL tables\n",
    "\n",
    "Our catalog is named prod."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be46061-f82f-4701-9278-da5c6d7347be",
   "metadata": {},
   "source": [
    "## Upstream tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "04d53262-eebc-4f7e-875b-27d2755a1943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|     prod|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bdd8dba6-b604-49e2-b9c1-95872918d582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS prod.db.orders\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS prod.db.orders\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS prod.db.orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0d0fba84-5e3f-4d6f-b2ce-b4cafe7cd857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Table DDL for OLTP tables\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS prod.db.customer (\n",
    "    customer_id INT,\n",
    "    email STRING,\n",
    "    first_name STRING,\n",
    "    last_name STRING,\n",
    "    phone STRING,\n",
    "    status STRING,\n",
    "    is_verified BOOLEAN,\n",
    "    registration_date TIMESTAMP,\n",
    "    last_login_date TIMESTAMP,\n",
    "    datetime_created TIMESTAMP,\n",
    "    datetime_updated TIMESTAMP\n",
    ") USING iceberg\n",
    "TBLPROPERTIES (\n",
    "    'format-version' = '2'\n",
    ")\"\"\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "-- Profile table (child to customer - many profiles per customer)\n",
    "CREATE TABLE IF NOT EXISTS prod.db.profile (\n",
    "    profile_id INT,\n",
    "    customer_id INT,\n",
    "    profile_name STRING,\n",
    "    profile_type STRING,\n",
    "    address_line1 STRING,\n",
    "    address_line2 STRING,\n",
    "    city STRING,\n",
    "    state STRING,\n",
    "    postal_code STRING,\n",
    "    country STRING,\n",
    "    is_default BOOLEAN,\n",
    "    datetime_created TIMESTAMP,\n",
    "    datetime_updated TIMESTAMP\n",
    ") USING iceberg\n",
    "TBLPROPERTIES (\n",
    "    'format-version' = '2'\n",
    ");\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "-- Orders table (references profile)\n",
    "CREATE TABLE IF NOT EXISTS prod.db.orders (\n",
    "    order_id INT,\n",
    "    customer_id INT,\n",
    "    order_date TIMESTAMP,\n",
    "    order_status STRING,\n",
    "    payment_method STRING,\n",
    "    payment_status STRING,\n",
    "    subtotal DECIMAL(10, 2),\n",
    "    tax_amount DECIMAL(10, 2),\n",
    "    shipping_amount DECIMAL(10, 2),\n",
    "    discount_amount DECIMAL(10, 2),\n",
    "    total_amount DECIMAL(10, 2),\n",
    "    currency STRING,\n",
    "    shipping_method STRING,\n",
    "    tracking_number STRING,\n",
    "    notes STRING,\n",
    "    datetime_created TIMESTAMP\n",
    ") USING iceberg\n",
    "TBLPROPERTIES (\n",
    "    'format-version' = '2'\n",
    ");\n",
    "\"\"\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "29d766d2-489b-4045-a4fe-94704428cc40",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[INSERT_COLUMN_ARITY_MISMATCH.NOT_ENOUGH_DATA_COLUMNS] Cannot write to `demo`.`prod`.`db`.`customer`, the reason is not enough data columns:\nTable columns: `customer_id`, `email`, `first_name`, `last_name`, `phone`, `status`, `is_verified`, `registration_date`, `last_login_date`, `datetime_created`, `datetime_updated`, `is_fraud`.\nData columns: `col1`, `col2`, `col3`, `col4`, `col5`, `col6`, `col7`, `col8`, `col9`, `col10`, `col11`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[131], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Insert some fake data for the OLTP tables\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43m-- Insert sample customers\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;43mINSERT INTO prod.db.customer VALUES\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;43m  (1, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjohn.doe@example.com\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mJohn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDoe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m555-123-4567\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mactive\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, true, TIMESTAMP \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2023-01-15 08:30:00\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, TIMESTAMP \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2023-03-20 14:22:15\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, TIMESTAMP \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2023-01-15 08:30:00\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, TIMESTAMP \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2023-03-20 14:22:15\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m),\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;43m  (2, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjane.smith@example.com\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mJane\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSmith\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m555-987-6543\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mactive\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, true, TIMESTAMP \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2023-02-05 12:45:00\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, TIMESTAMP \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2023-03-18 09:10:30\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, TIMESTAMP \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2023-02-05 12:45:00\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, TIMESTAMP \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2023-03-18 09:10:30\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m),\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;43m  (3, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrobert.brown@example.com\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRobert\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBrown\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m555-456-7890\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minactive\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, false, TIMESTAMP \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2023-01-25 15:20:00\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, TIMESTAMP \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2023-02-10 11:05:45\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, TIMESTAMP \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2023-01-25 15:20:00\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, TIMESTAMP \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2023-02-10 11:05:45\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m);\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m-- Insert sample profiles\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124mINSERT INTO prod.db.profile VALUES\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124m  (105, 3, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHome Address\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhome\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m202 Cedar Lane\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, NULL, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChicago\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIL\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m60601\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUS\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, true, TIMESTAMP \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2023-01-25 15:25:00\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, TIMESTAMP \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2023-01-25 15:25:00\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m);\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124m-- Insert sample orders\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124mINSERT INTO prod.db.orders VALUES\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124m  (1005, 3, TIMESTAMP \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2023-02-05 11:10:00\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcancelled\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcredit_card\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrefunded\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, 199.99, 16.00, 12.99, 20.00, 208.98, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUSD\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124movernight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, NULL, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCustomer requested cancellation\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, TIMESTAMP \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2023-02-05 11:10:00\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m);\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124m  \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [INSERT_COLUMN_ARITY_MISMATCH.NOT_ENOUGH_DATA_COLUMNS] Cannot write to `demo`.`prod`.`db`.`customer`, the reason is not enough data columns:\nTable columns: `customer_id`, `email`, `first_name`, `last_name`, `phone`, `status`, `is_verified`, `registration_date`, `last_login_date`, `datetime_created`, `datetime_updated`, `is_fraud`.\nData columns: `col1`, `col2`, `col3`, `col4`, `col5`, `col6`, `col7`, `col8`, `col9`, `col10`, `col11`."
     ]
    }
   ],
   "source": [
    "# Insert some fake data for the OLTP tables\n",
    "spark.sql(\"\"\"\n",
    "-- Insert sample customers\n",
    "INSERT INTO prod.db.customer VALUES\n",
    "  (1, 'john.doe@example.com', 'John', 'Doe', '555-123-4567', 'active', true, TIMESTAMP '2023-01-15 08:30:00', TIMESTAMP '2023-03-20 14:22:15', TIMESTAMP '2023-01-15 08:30:00', TIMESTAMP '2023-03-20 14:22:15'),\n",
    "  (2, 'jane.smith@example.com', 'Jane', 'Smith', '555-987-6543', 'active', true, TIMESTAMP '2023-02-05 12:45:00', TIMESTAMP '2023-03-18 09:10:30', TIMESTAMP '2023-02-05 12:45:00', TIMESTAMP '2023-03-18 09:10:30'),\n",
    "  (3, 'robert.brown@example.com', 'Robert', 'Brown', '555-456-7890', 'inactive', false, TIMESTAMP '2023-01-25 15:20:00', TIMESTAMP '2023-02-10 11:05:45', TIMESTAMP '2023-01-25 15:20:00', TIMESTAMP '2023-02-10 11:05:45');\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "-- Insert sample profiles\n",
    "INSERT INTO prod.db.profile VALUES\n",
    "  (101, 1, 'Home Address', 'home', '123 Main St', 'Apt 4B', 'New York', 'NY', '10001', 'US', true, TIMESTAMP '2023-01-15 08:35:00', TIMESTAMP '2023-01-15 08:35:00'),\n",
    "  (102, 1, 'Work Address', 'work', '456 Business Ave', 'Suite 200', 'New York', 'NY', '10002', 'US', false, TIMESTAMP '2023-02-10 09:20:00', TIMESTAMP '2023-02-10 09:20:00'),\n",
    "  (103, 2, 'Shipping Address', 'shipping', '789 Residential Blvd', NULL, 'Los Angeles', 'CA', '90001', 'US', true, TIMESTAMP '2023-02-05 12:50:00', TIMESTAMP '2023-02-05 12:50:00'),\n",
    "  (104, 2, 'Billing Address', 'billing', '101 Finance St', '15th Floor', 'Los Angeles', 'CA', '90002', 'US', false, TIMESTAMP '2023-02-05 12:55:00', TIMESTAMP '2023-02-05 12:55:00'),\n",
    "  (105, 3, 'Home Address', 'home', '202 Cedar Lane', NULL, 'Chicago', 'IL', '60601', 'US', true, TIMESTAMP '2023-01-25 15:25:00', TIMESTAMP '2023-01-25 15:25:00');\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "-- Insert sample orders\n",
    "INSERT INTO prod.db.orders VALUES\n",
    "  (1001, 1, TIMESTAMP '2023-02-01 10:15:00', 'delivered', 'credit_card', 'paid', 89.99, 7.20, 5.99, 0.00, 103.18, 'USD', 'standard', 'TRK123456789', NULL, TIMESTAMP '2023-02-01 10:15:00'),\n",
    "  (1002, 1, TIMESTAMP '2023-03-10 14:30:00', 'shipped', 'paypal', 'paid', 45.50, 3.64, 5.99, 5.00, 50.13, 'USD', 'express', 'TRK987654321', NULL, TIMESTAMP '2023-03-10 14:30:00'),\n",
    "  (1003, 2, TIMESTAMP '2023-02-20 09:45:00', 'delivered', 'credit_card', 'paid', 129.95, 10.40, 9.99, 15.00, 135.34, 'USD', 'standard', 'TRK456789123', 'Please leave at front door', TIMESTAMP '2023-02-20 09:45:00'),\n",
    "  (1004, 2, TIMESTAMP '2023-03-15 16:20:00', 'processing', 'apple_pay', 'paid', 75.25, 6.02, 5.99, 0.00, 87.26, 'USD', 'standard', NULL, NULL, TIMESTAMP '2023-03-15 16:20:00'),\n",
    "  (1005, 3, TIMESTAMP '2023-02-05 11:10:00', 'cancelled', 'credit_card', 'refunded', 199.99, 16.00, 12.99, 20.00, 208.98, 'USD', 'overnight', NULL, 'Customer requested cancellation', TIMESTAMP '2023-02-05 11:10:00');\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16273753-d688-4fc3-9362-34b7f1fab1ea",
   "metadata": {},
   "source": [
    "## Dimensional tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8ff638ac-efcb-497f-a087-4531bd0a39b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS prod.db.dim_customer\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS prod.db.fct_orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4dd4c209-26fe-44e9-9d29-d1f2ecaeb7a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "-- Customer Dimension Table with profiles as array of structs\n",
    "CREATE TABLE IF NOT EXISTS prod.db.dim_customer (\n",
    "    customer_id INT,\n",
    "    email STRING,\n",
    "    first_name STRING,\n",
    "    last_name STRING,\n",
    "    phone STRING,\n",
    "    status STRING,\n",
    "    is_verified BOOLEAN,\n",
    "    registration_date TIMESTAMP,\n",
    "    last_login_date TIMESTAMP,\n",
    "    \n",
    "    -- Profiles as array of structs\n",
    "    profiles ARRAY<STRUCT<\n",
    "        profile_id: INT,\n",
    "        profile_name: STRING,\n",
    "        profile_type: STRING,\n",
    "        address_line1: STRING,\n",
    "        address_line2: STRING,\n",
    "        city: STRING,\n",
    "        state: STRING,\n",
    "        postal_code: STRING,\n",
    "        country: STRING,\n",
    "        is_default: BOOLEAN,\n",
    "        datetime_created: TIMESTAMP,\n",
    "        datetime_updated: TIMESTAMP\n",
    "    >>,\n",
    "    \n",
    "    datetime_created TIMESTAMP,\n",
    "    datetime_updated TIMESTAMP,\n",
    "    etl_upserted TIMESTAMP\n",
    ") USING iceberg\n",
    "PARTITIONED BY (datetime_updated)\n",
    "TBLPROPERTIES (\n",
    "    'format-version' = '2',\n",
    "    'write.spark.accept-any-schema'='true'\n",
    ");\"\"\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "-- Orders Fact Table\n",
    "CREATE TABLE IF NOT EXISTS prod.db.fct_orders (\n",
    "    order_id INT,\n",
    "    \n",
    "    -- Foreign keys\n",
    "    customer_id INT,\n",
    "    \n",
    "    -- Time dimensions\n",
    "    order_date TIMESTAMP,\n",
    "    \n",
    "    -- Order attributes\n",
    "    order_status STRING,\n",
    "    payment_method STRING,\n",
    "    payment_status STRING,\n",
    "    \n",
    "    -- Order metrics\n",
    "    subtotal DECIMAL(10, 2),\n",
    "    tax_amount DECIMAL(10, 2),\n",
    "    shipping_amount DECIMAL(10, 2),\n",
    "    discount_amount DECIMAL(10, 2),\n",
    "    total_amount DECIMAL(10, 2),\n",
    "    \n",
    "    -- Order details\n",
    "    currency STRING,\n",
    "    shipping_method STRING,\n",
    "    tracking_number STRING,\n",
    "    notes STRING\n",
    "\n",
    ") USING iceberg\n",
    "TBLPROPERTIES (\n",
    "    'format-version' = '2'\n",
    ");\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258a38f3-836d-4d4d-b181-94f627019c92",
   "metadata": {},
   "source": [
    "## Gold tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514b0d2b-1644-407d-a32d-1984784e12c2",
   "metadata": {},
   "source": [
    "### OBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "615cdf88-f013-492b-8874-f51265c683b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS prod.db.obt_orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d1ab320c-dc83-4c33-965f-71d4d6cd81ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Order Business Table (OBT) - Denormalized view of orders with customer information\n",
    "spark.sql(\"\"\" \n",
    "\n",
    "CREATE TABLE IF NOT EXISTS prod.db.obt_orders (\n",
    "    -- Order primary key\n",
    "    order_id INT,\n",
    "    \n",
    "    -- Order date and time attributes\n",
    "    order_date TIMESTAMP,\n",
    "    \n",
    "    -- Order attributes\n",
    "    order_status STRING,\n",
    "    payment_method STRING,\n",
    "    payment_status STRING,\n",
    "    currency STRING,\n",
    "    shipping_method STRING,\n",
    "    tracking_number STRING,\n",
    "    notes STRING,\n",
    "    \n",
    "    -- Order metrics\n",
    "    subtotal DECIMAL(10, 2),\n",
    "    tax_amount DECIMAL(10, 2),\n",
    "    shipping_amount DECIMAL(10, 2),\n",
    "    discount_amount DECIMAL(10, 2),\n",
    "    total_amount DECIMAL(10, 2),\n",
    "    \n",
    "    -- Customer information\n",
    "    \n",
    "    customer STRUCT<\n",
    "    customer_id: INT,\n",
    "    email: STRING,\n",
    "    first_name: STRING,\n",
    "    last_name: STRING,\n",
    "    phone: STRING,\n",
    "    status: STRING,\n",
    "    is_verified: BOOLEAN,\n",
    "    registration_date: TIMESTAMP,\n",
    "    last_login_date: TIMESTAMP,\n",
    "    datetime_created TIMESTAMP,\n",
    "    datetime_updated TIMESTAMP,\n",
    "    etl_upserted TIMESTAMP,\n",
    "    \n",
    "    -- Profiles as array of structs\n",
    "    profiles ARRAY<STRUCT<\n",
    "        profile_id: INT,\n",
    "        profile_name: STRING,\n",
    "        profile_type: STRING,\n",
    "        address_line1: STRING,\n",
    "        address_line2: STRING,\n",
    "        city: STRING,\n",
    "        state: STRING,\n",
    "        postal_code: STRING,\n",
    "        country: STRING,\n",
    "        is_default: BOOLEAN,\n",
    "        datetime_created: TIMESTAMP,\n",
    "        datetime_updated: TIMESTAMP\n",
    "    >>\n",
    "    >,\n",
    "    \n",
    "    -- ETL metadata\n",
    "    datetime_created TIMESTAMP,\n",
    "    etl_upserted TIMESTAMP\n",
    ") USING iceberg\n",
    "PARTITIONED BY (datetime_created)\n",
    "TBLPROPERTIES (\n",
    "    'format-version' = '2',\n",
    "    'write.spark.accept-any-schema'='true'\n",
    ");\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae879ba-59ab-4fcf-a46a-e165e3b09efa",
   "metadata": {},
   "source": [
    "# Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ab7fe3-389f-4a48-b0ed-6b398b10f928",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Schema evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b30e2e30-e592-419d-b37d-2104489ddb76",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "customer_df = spark.table(\"prod.db.customer\")\n",
    "profile_df = spark.table(\"prod.db.profile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bff057be-90d0-423d-a0f5-a5aad6f51e1c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "be768331-3718-4306-87c7-2fab4b5bb48f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['profile_id',\n",
       " 'customer_id',\n",
       " 'profile_name',\n",
       " 'profile_type',\n",
       " 'address_line1',\n",
       " 'address_line2',\n",
       " 'city',\n",
       " 'state',\n",
       " 'postal_code',\n",
       " 'country',\n",
       " 'is_default',\n",
       " 'datetime_created',\n",
       " 'datetime_updated']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ffa4d313-25f8-48cc-994b-7c6024f32ed8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['profile_id',\n",
       " 'profile_name',\n",
       " 'profile_type',\n",
       " 'address_line1',\n",
       " 'address_line2',\n",
       " 'city',\n",
       " 'state',\n",
       " 'postal_code',\n",
       " 'country',\n",
       " 'is_default',\n",
       " 'datetime_created',\n",
       " 'datetime_updated']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_columns = [c for c in profile_df.columns if 'customer_id' not in c]\n",
    "new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ff2f0d33-df95-4e44-b8c4-4d0364113daa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['profile_id',\n",
       " 'profile_type',\n",
       " 'address_line2',\n",
       " 'state',\n",
       " 'country',\n",
       " 'datetime_created']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_cols = new_columns[0:11:2]\n",
    "existing_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cf119244-e45f-4a2e-9af0-ae5baad30a86",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['postal_code',\n",
       " 'profile_name',\n",
       " 'city',\n",
       " 'datetime_updated',\n",
       " 'is_default',\n",
       " 'address_line1']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_add = list(set(new_columns) - set(existing_cols))\n",
    "cols_to_add # alter table; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ccc179bc-efce-4644-ab98-2ca2cd69c898",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['profile_id',\n",
       " 'profile_type',\n",
       " 'address_line2',\n",
       " 'state',\n",
       " 'country',\n",
       " 'datetime_created',\n",
       " 'postal_code',\n",
       " 'profile_name',\n",
       " 'city',\n",
       " 'datetime_updated',\n",
       " 'is_default',\n",
       " 'address_line1']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_cols_order = existing_cols + cols_to_add\n",
    "new_cols_order # if existing cols are removed; they will be null going forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "40b5ae47-5451-424a-b50e-27e537be9ed6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|customer_id|profile_info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "+-----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1          |[{123 Main St, Apt 4B, New York, US, 2023-01-15 08:35:00, 2023-01-15 08:35:00, true, 10001, 101, Home Address, home, NY}, {456 Business Ave, Suite 200, New York, US, 2023-02-10 09:20:00, 2023-02-10 09:20:00, false, 10002, 102, Work Address, work, NY}, {123 Main St, Apt 4B, New York, US, 2023-01-15 08:35:00, 2023-01-15 08:35:00, true, 10001, 101, Home Address, home, NY}, {456 Business Ave, Suite 200, New York, US, 2023-02-10 09:20:00, 2023-02-10 09:20:00, false, 10002, 102, Work Address, work, NY}]                                                    |\n",
      "|2          |[{789 Residential Blvd, NULL, Los Angeles, US, 2023-02-05 12:50:00, 2023-02-05 12:50:00, true, 90001, 103, Shipping Address, shipping, CA}, {101 Finance St, 15th Floor, Los Angeles, US, 2023-02-05 12:55:00, 2023-02-05 12:55:00, false, 90002, 104, Billing Address, billing, CA}, {789 Residential Blvd, NULL, Los Angeles, US, 2023-02-05 12:50:00, 2023-02-05 12:50:00, true, 90001, 103, Shipping Address, shipping, CA}, {101 Finance St, 15th Floor, Los Angeles, US, 2023-02-05 12:55:00, 2023-02-05 12:55:00, false, 90002, 104, Billing Address, billing, CA}]|\n",
      "|3          |[{202 Cedar Lane, NULL, Chicago, US, 2023-01-25 15:25:00, 2023-01-25 15:25:00, true, 60601, 105, Home Address, home, IL}, {202 Cedar Lane, NULL, Chicago, US, 2023-01-25 15:25:00, 2023-01-25 15:25:00, true, 60601, 105, Home Address, home, IL}]                                                                                                                                                                                                                                                                                                                        |\n",
      "+-----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profile_df.groupBy(profile_df.customer_id).agg(\n",
    "    F.collect_list(\n",
    "        F.struct(sorted([c for c in profile_df.columns if 'customer_id' not in c]))\n",
    "    ).alias(\"profile_info\")\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "161cb711-4eaf-48bf-a846-353c477cc9e9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['profile_id',\n",
       " 'profile_name',\n",
       " 'profile_type',\n",
       " 'address_line1',\n",
       " 'address_line2',\n",
       " 'city',\n",
       " 'state',\n",
       " 'postal_code',\n",
       " 'country',\n",
       " 'is_default',\n",
       " 'datetime_created',\n",
       " 'datetime_updated']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_customer = spark.table(\"prod.db.dim_customer\")\n",
    "[c.name for c in dim_customer.select(\"profiles\").schema.fields[0].dataType.elementType.fields] # get existing schema\n",
    "#[elt for elt in [r.dataType for r in dim_customer.select(\"profiles\").schema]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8bc2f651-0f3f-4c22-b957-082c2d001be6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_struct_cols_to_add(new_col_list, old_col_list):\n",
    "    return sorted(list(set(new_col_list) - set(old_col_list)))\n",
    "\n",
    "last_col = old_col_list[-1]\n",
    "for new_col in get_struct_cols_to_add(new_col_list, old_col_list):\n",
    "    ddl = f\"ALTER TABLE prod.db.sample ADD COLUMN nested.new_column bigint FIRST\"\n",
    "    spark.sql(ddl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcdda088-ba0e-4e72-bb88-fdf6d1e81919",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('profile_id', 'integer'),\n",
       " ('profile_name', 'string'),\n",
       " ('profile_type', 'string'),\n",
       " ('address_line1', 'string'),\n",
       " ('address_line2', 'string'),\n",
       " ('city', 'string'),\n",
       " ('state', 'string'),\n",
       " ('postal_code', 'string'),\n",
       " ('country', 'string'),\n",
       " ('is_default', 'boolean'),\n",
       " ('datetime_created', 'timestamp'),\n",
       " ('datetime_updated', 'timestamp')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(c.jsonValue().get('name'), c.jsonValue().get('type')) for c in spark.table(\"prod.db.dim_customer\").select(\"profiles\").schema.fields[0].dataType.elementType.fields] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611752b8-975c-4ee3-8adb-ceaccf838630",
   "metadata": {},
   "source": [
    "## ETL Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4744ebd8-c4e0-4e4f-abca-75b37572408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# Set at session level\n",
    "spark.conf.set(\"spark.sql.iceberg.write.accept-any-schema\", \"true\")\n",
    "\n",
    "class DimCustomer:\n",
    "    def extract_upstream(self, start_ts, end_ts):\n",
    "        print(\"Starting EXRACT...\")\n",
    "        customer_df = spark.table(\"prod.db.customer\").where(F.col(\"datetime_updated\").between(start_ts, end_ts))\n",
    "        profile_df = spark.table(\"prod.db.profile\").where(F.col(\"datetime_updated\").between(start_ts, end_ts))\n",
    "        return {\n",
    "            \"customer\": customer_df,\n",
    "            \"profile\": profile_df\n",
    "        }\n",
    "    \n",
    "    def transform(self, input_dfs):\n",
    "        print(\"Starting TRANSFORM...\")\n",
    "        customer_df = input_dfs.get(\"customer\")\n",
    "        profile_df = input_dfs.get(\"profile\")\n",
    "        profile_struct_columns = [c for c in profile_df.columns if 'customer_id' not in c]\n",
    "\n",
    "        customer_cols = customer_df.columns\n",
    "\n",
    "        grouped_profile_id = profile_df.groupBy(\n",
    "            F.col(\"customer_id\")\n",
    "        ).agg(\n",
    "            F.collect_list(\n",
    "                F.struct(profile_struct_columns)\n",
    "            )\n",
    "        .alias(\"profiles\"))\n",
    "        \n",
    "        return customer_df.join(\n",
    "            grouped_profile_id,\n",
    "            on=\"customer_id\"\n",
    "        ).select(\n",
    "            *customer_cols\n",
    "            ,grouped_profile_id['profiles']\n",
    "        )\n",
    "    \n",
    "    def load(self, transformed_df):\n",
    "        print(\"Starting LOAD...\")\n",
    "        existing_struct_cols = [c.name for c in spark.table(\"prod.db.dim_customer\").select(\"profiles\").schema.fields[0].dataType.elementType.fields] # get existing schema\n",
    "        #new_struct_cols = [c.name for c in transformed_df.select(\"profiles\").schema.fields[0].dataType.elementType.fields]\n",
    "        new_col_data_types = [(c.jsonValue().get('name'), c.jsonValue().get('type')) for c in transformed_df.select(\"profiles\").schema.fields[0].dataType.elementType.fields] \n",
    "\n",
    "        #new_cols =  sorted(list(set(new_struct_cols) - set(existing_struct_cols)))\n",
    "        \n",
    "        # last_col = existing_struct_cols[-1]\n",
    "        # for new_col, data_type in new_col_data_types:\n",
    "        #     if new_col not in existing_struct_cols:\n",
    "        #         print(f'Updating dim_customer profiles schema...adding {new_col}')\n",
    "        #         ddl = f\"ALTER TABLE prod.db.dim_customer ADD COLUMN profiles.element.{new_col} {data_type} AFTER {last_col}\"\n",
    "        #         spark.sql(ddl)\n",
    "        #         last_col = new_col\n",
    "\n",
    "        print(\"Loading in data...\")\n",
    "        # Reorder columns to match the table schema\n",
    "        column_order = [\n",
    "            \"customer_id\", \"email\", \"first_name\", \"last_name\", \"phone\", \n",
    "            \"status\", \"is_verified\", \"registration_date\", \"last_login_date\",\n",
    "            \"profiles\", \"datetime_created\", \"datetime_updated\", \"etl_upserted\", \"is_fraud\"\n",
    "        ]\n",
    "        \n",
    "        # Select columns in the correct order\n",
    "        transformed_df = transformed_df.withColumn(\"etl_upserted\", F.current_timestamp()).select(column_order)\n",
    "\n",
    "        transformed_df\\\n",
    "        .writeTo(\"prod.db.dim_customer\")\\\n",
    "        .option(\"mergeSchema\", \"true\")\\\n",
    "        .overwritePartitions()\n",
    "        # .when(matched=\"update all\") \\\n",
    "        # .when(notMatched=\"insert *\") \\\n",
    "        # .execute()\n",
    "\n",
    "        # transformed_df.createOrReplaceTempView(\"tmp_dim_customer\")\n",
    "        # spark.sql(\"\"\"\n",
    "        # MERGE INTO prod.db.dim_customer t\n",
    "        # USING (SELECT * FROM tmp_dim_customer) s\n",
    "        # ON t.customer_id = s.customer_id\n",
    "        # WHEN MATCHED THEN UPDATE SET *\n",
    "        # WHEN NOT MATCHED THEN INSERT * \n",
    "        # \"\"\")\n",
    "        \n",
    "    \n",
    "    def run(self, start_ts, end_ts):\n",
    "        print(\"Starting RUN...\")\n",
    "        # Log: run_time, start_ts, end_ts, START state\n",
    "        self.load(self.transform(self.extract_upstream(start_ts, end_ts)))\n",
    "        # Log: run_time, start_ts, end_ts, END state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6726dc9c-df5c-437f-81bd-07dbb60a9937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RUN...\n",
      "Starting EXRACT...\n",
      "Starting TRANSFORM...\n",
      "Starting LOAD...\n",
      "Loading in data...\n"
     ]
    }
   ],
   "source": [
    "dim_customer = DimCustomer()\n",
    "dim_customer.run('2022-01-01', '2024-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "76daab63-e098-4d4a-99e9-263f4ec33b5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DimCustomer' object has no attribute 'printSchema'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdim_customer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprintSchema\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DimCustomer' object has no attribute 'printSchema'"
     ]
    }
   ],
   "source": [
    "dim_customer.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefef438-f684-4f7f-9b58-7231dd5ae360",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"Alter table prod.db.profile add column is_under_18 boolean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92171b02-ece2-4440-bbb6-513b00958255",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_customer = DimCustomer()\n",
    "dim_customer.run('2022-01-01', '2024-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "144d9cf3-e195-41f6-aefe-ec6ab2fa3a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OBTOrders:\n",
    "    def extract_upstream(self, start_ts, end_ts):\n",
    "        print(\"Starting EXTRACT...\")\n",
    "        fct_orders = spark.table(\"prod.db.orders\").where(F.col(\"datetime_created\").between(start_ts, end_ts))\n",
    "        dim_customer = spark.table(\"prod.db.dim_customer\").where(F.col(\"datetime_updated\").between(start_ts, end_ts))\n",
    "        return {\n",
    "            \"fct_orders\": fct_orders,\n",
    "            \"dim_customer\": dim_customer\n",
    "        }\n",
    "    \n",
    "    def transform(self, input_dfs):\n",
    "        print(\"Starting TRANSFORM...\")\n",
    "        fct_orders = input_dfs.get(\"fct_orders\")\n",
    "        dim_customer = input_dfs.get(\"dim_customer\")\n",
    "\n",
    "        order_cols = fct_orders.columns\n",
    "        dim_customer_cols = dim_customer.columns\n",
    "        \n",
    "        return fct_orders.join(\n",
    "            dim_customer,\n",
    "            on=\"customer_id\"\n",
    "        ).select(\n",
    "            *[fct_orders[order_col] for order_col in order_cols]\n",
    "            , F.struct(\n",
    "                *[dim_customer[dim_customer_col] for dim_customer_col in dim_customer_cols]\n",
    "            ).alias(\"customer\")\n",
    "        ).drop(\"customer_id\")\n",
    "    \n",
    "    def load(self, transformed_df):\n",
    "        print(\"Starting LOAD...\")\n",
    "        transformed_df.withColumn(\"etl_upserted\", F.current_timestamp()).writeTo(\"prod.db.obt_orders\").option(\"mergeSchema\", \"true\").overwritePartitions()\n",
    "        \n",
    "    \n",
    "    def run(self, start_ts, end_ts):\n",
    "        print(\"Starting RUN...\")\n",
    "        # Log: run_time, start_ts, end_ts, START state\n",
    "        self.load(self.transform(self.extract_upstream(start_ts, end_ts)))\n",
    "        # Log: run_time, start_ts, end_ts, END state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2563f89b-61f6-4784-91c3-ef0238722f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RUN...\n",
      "Starting EXTRACT...\n",
      "Starting TRANSFORM...\n",
      "Starting LOAD...\n"
     ]
    }
   ],
   "source": [
    "obt_orders = OBTOrders()\n",
    "obt_orders.run('2022-01-01', '2024-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f9cd1f13-f039-49be-b428-120053114498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+------------+--------------+--------------+--------+---------------+---------------+-------------------------------+--------+----------+---------------+---------------+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+--------------------------+\n",
      "|order_id|order_date         |order_status|payment_method|payment_status|currency|shipping_method|tracking_number|notes                          |subtotal|tax_amount|shipping_amount|discount_amount|total_amount|customer                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |datetime_created   |etl_upserted              |\n",
      "+--------+-------------------+------------+--------------+--------------+--------+---------------+---------------+-------------------------------+--------+----------+---------------+---------------+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+--------------------------+\n",
      "|1003    |2023-02-20 09:45:00|delivered   |credit_card   |paid          |USD     |standard       |TRK456789123   |Please leave at front door     |129.95  |10.40     |9.99           |15.00          |135.34      |{2, jane.smith@example.com, Jane, Smith, 555-987-6543, active, true, 2023-02-05 12:45:00, 2023-03-18 09:10:30, 2023-02-05 12:45:00, 2023-03-18 09:10:30, 2025-03-22 20:05:19.628954, [{103, Shipping Address, shipping, 789 Residential Blvd, NULL, Los Angeles, CA, 90001, US, true, 2023-02-05 12:50:00, 2023-02-05 12:50:00}, {104, Billing Address, billing, 101 Finance St, 15th Floor, Los Angeles, CA, 90002, US, false, 2023-02-05 12:55:00, 2023-02-05 12:55:00}]}|2023-02-20 09:45:00|2025-03-22 20:05:20.989753|\n",
      "|1002    |2023-03-10 14:30:00|shipped     |paypal        |paid          |USD     |express        |TRK987654321   |NULL                           |45.50   |3.64      |5.99           |5.00           |50.13       |{1, john.doe@example.com, John, Doe, 555-123-4567, active, true, 2023-01-15 08:30:00, 2023-03-20 14:22:15, 2023-01-15 08:30:00, 2023-03-20 14:22:15, 2025-03-22 20:05:19.628954, [{101, Home Address, home, 123 Main St, Apt 4B, New York, NY, 10001, US, true, 2023-01-15 08:35:00, 2023-01-15 08:35:00}, {102, Work Address, work, 456 Business Ave, Suite 200, New York, NY, 10002, US, false, 2023-02-10 09:20:00, 2023-02-10 09:20:00}]}                              |2023-03-10 14:30:00|2025-03-22 20:05:20.989753|\n",
      "|1005    |2023-02-05 11:10:00|cancelled   |credit_card   |refunded      |USD     |overnight      |NULL           |Customer requested cancellation|199.99  |16.00     |12.99          |20.00          |208.98      |{3, robert.brown@example.com, Robert, Brown, 555-456-7890, inactive, false, 2023-01-25 15:20:00, 2023-02-10 11:05:45, 2023-01-25 15:20:00, 2023-02-10 11:05:45, 2025-03-22 20:05:19.628954, [{105, Home Address, home, 202 Cedar Lane, NULL, Chicago, IL, 60601, US, true, 2023-01-25 15:25:00, 2023-01-25 15:25:00}]}                                                                                                                                                     |2023-02-05 11:10:00|2025-03-22 20:05:20.989753|\n",
      "|1001    |2023-02-01 10:15:00|delivered   |credit_card   |paid          |USD     |standard       |TRK123456789   |NULL                           |89.99   |7.20      |5.99           |0.00           |103.18      |{1, john.doe@example.com, John, Doe, 555-123-4567, active, true, 2023-01-15 08:30:00, 2023-03-20 14:22:15, 2023-01-15 08:30:00, 2023-03-20 14:22:15, 2025-03-22 20:05:19.628954, [{101, Home Address, home, 123 Main St, Apt 4B, New York, NY, 10001, US, true, 2023-01-15 08:35:00, 2023-01-15 08:35:00}, {102, Work Address, work, 456 Business Ave, Suite 200, New York, NY, 10002, US, false, 2023-02-10 09:20:00, 2023-02-10 09:20:00}]}                              |2023-02-01 10:15:00|2025-03-22 20:05:20.989753|\n",
      "|1004    |2023-03-15 16:20:00|processing  |apple_pay     |paid          |USD     |standard       |NULL           |NULL                           |75.25   |6.02      |5.99           |0.00           |87.26       |{2, jane.smith@example.com, Jane, Smith, 555-987-6543, active, true, 2023-02-05 12:45:00, 2023-03-18 09:10:30, 2023-02-05 12:45:00, 2023-03-18 09:10:30, 2025-03-22 20:05:19.628954, [{103, Shipping Address, shipping, 789 Residential Blvd, NULL, Los Angeles, CA, 90001, US, true, 2023-02-05 12:50:00, 2023-02-05 12:50:00}, {104, Billing Address, billing, 101 Finance St, 15th Floor, Los Angeles, CA, 90002, US, false, 2023-02-05 12:55:00, 2023-02-05 12:55:00}]}|2023-03-15 16:20:00|2025-03-22 20:05:20.989753|\n",
      "+--------+-------------------+------------+--------------+--------------+--------+---------------+---------------+-------------------------------+--------+----------+---------------+---------------+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from prod.db.obt_orders\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "22b3a42d-287b-4551-aefd-80674f6fae0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- payment_status: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- shipping_method: string (nullable = true)\n",
      " |-- tracking_number: string (nullable = true)\n",
      " |-- notes: string (nullable = true)\n",
      " |-- subtotal: decimal(10,2) (nullable = true)\n",
      " |-- tax_amount: decimal(10,2) (nullable = true)\n",
      " |-- shipping_amount: decimal(10,2) (nullable = true)\n",
      " |-- discount_amount: decimal(10,2) (nullable = true)\n",
      " |-- total_amount: decimal(10,2) (nullable = true)\n",
      " |-- customer: struct (nullable = true)\n",
      " |    |-- customer_id: integer (nullable = true)\n",
      " |    |-- email: string (nullable = true)\n",
      " |    |-- first_name: string (nullable = true)\n",
      " |    |-- last_name: string (nullable = true)\n",
      " |    |-- phone: string (nullable = true)\n",
      " |    |-- status: string (nullable = true)\n",
      " |    |-- is_verified: boolean (nullable = true)\n",
      " |    |-- registration_date: timestamp (nullable = true)\n",
      " |    |-- last_login_date: timestamp (nullable = true)\n",
      " |    |-- datetime_created: timestamp (nullable = true)\n",
      " |    |-- datetime_updated: timestamp (nullable = true)\n",
      " |    |-- etl_upserted: timestamp (nullable = true)\n",
      " |    |-- profiles: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- profile_id: integer (nullable = true)\n",
      " |    |    |    |-- profile_name: string (nullable = true)\n",
      " |    |    |    |-- profile_type: string (nullable = true)\n",
      " |    |    |    |-- address_line1: string (nullable = true)\n",
      " |    |    |    |-- address_line2: string (nullable = true)\n",
      " |    |    |    |-- city: string (nullable = true)\n",
      " |    |    |    |-- state: string (nullable = true)\n",
      " |    |    |    |-- postal_code: string (nullable = true)\n",
      " |    |    |    |-- country: string (nullable = true)\n",
      " |    |    |    |-- is_default: boolean (nullable = true)\n",
      " |    |    |    |-- datetime_created: timestamp (nullable = true)\n",
      " |    |    |    |-- datetime_updated: timestamp (nullable = true)\n",
      " |-- datetime_created: timestamp (nullable = true)\n",
      " |-- etl_upserted: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(\"prod.db.obt_orders\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0d338bb1-4a2f-465e-a1c0-cc797614e1ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's add a column to customer and a column to profile in the source system and see how it flows through\n",
    "spark.sql(\"Alter table prod.db.customer add column is_fraud boolean\")\n",
    "spark.sql(\"Alter table prod.db.profile add column is_under_18 boolean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b730f0ac-e694-4130-9635-885268a27c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RUN...\n",
      "Starting EXRACT...\n",
      "Starting TRANSFORM...\n",
      "Starting LOAD...\n",
      "Updating dim_customer profiles schema...adding is_under_18\n",
      "Loading in data...\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[INSERT_COLUMN_ARITY_MISMATCH.TOO_MANY_DATA_COLUMNS] Cannot write to `demo`.`prod`.`db`.`dim_customer`, the reason is too many data columns:\nTable columns: `customer_id`, `email`, `first_name`, `last_name`, `phone`, `status`, `is_verified`, `registration_date`, `last_login_date`, `profiles`, `datetime_created`, `datetime_updated`, `etl_upserted`.\nData columns: `customer_id`, `email`, `first_name`, `last_name`, `phone`, `status`, `is_verified`, `registration_date`, `last_login_date`, `datetime_created`, `datetime_updated`, `is_fraud`, `profiles`, `etl_upserted`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# re-run ETL\u001b[39;00m\n\u001b[1;32m      2\u001b[0m dim_customer \u001b[38;5;241m=\u001b[39m DimCustomer()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdim_customer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2022-01-01\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2024-01-01\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m obt_orders \u001b[38;5;241m=\u001b[39m OBTOrders()\n\u001b[1;32m      6\u001b[0m obt_orders\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2022-01-01\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2024-01-01\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[67], line 60\u001b[0m, in \u001b[0;36mDimCustomer.run\u001b[0;34m(self, start_ts, end_ts)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting RUN...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Log: run_time, start_ts, end_ts, START state\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_upstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_ts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_ts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[67], line 54\u001b[0m, in \u001b[0;36mDimCustomer.load\u001b[0;34m(self, transformed_df)\u001b[0m\n\u001b[1;32m     51\u001b[0m         last_col \u001b[38;5;241m=\u001b[39m new_col\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading in data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m \u001b[43mtransformed_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43metl_upserted\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_timestamp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteTo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprod.db.dim_customer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmergeSchema\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverwritePartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py:2127\u001b[0m, in \u001b[0;36mDataFrameWriterV2.overwritePartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2118\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;241m3.1\u001b[39m)\n\u001b[1;32m   2119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moverwritePartitions\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2121\u001b[0m \u001b[38;5;124;03m    Overwrite all partition for which the data frame contains at least one row with the contents\u001b[39;00m\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;124;03m    of the data frame in the output table.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2125\u001b[0m \u001b[38;5;124;03m    partitions dynamically depending on the contents of the data frame.\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2127\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverwritePartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [INSERT_COLUMN_ARITY_MISMATCH.TOO_MANY_DATA_COLUMNS] Cannot write to `demo`.`prod`.`db`.`dim_customer`, the reason is too many data columns:\nTable columns: `customer_id`, `email`, `first_name`, `last_name`, `phone`, `status`, `is_verified`, `registration_date`, `last_login_date`, `profiles`, `datetime_created`, `datetime_updated`, `etl_upserted`.\nData columns: `customer_id`, `email`, `first_name`, `last_name`, `phone`, `status`, `is_verified`, `registration_date`, `last_login_date`, `datetime_created`, `datetime_updated`, `is_fraud`, `profiles`, `etl_upserted`."
     ]
    }
   ],
   "source": [
    "# re-run ETL\n",
    "dim_customer = DimCustomer()\n",
    "dim_customer.run('2022-01-01', '2024-01-01')\n",
    "\n",
    "obt_orders = OBTOrders()\n",
    "obt_orders.run('2022-01-01', '2024-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d77fe45-000f-4549-a061-93eac50291cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check new schema\n",
    "spark.table(\"prod.db.obt_orders\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa304c5-e8e6-412a-a0b6-fd71dc2f49d9",
   "metadata": {},
   "source": [
    "TODO: Add partition definitions to DDL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de00360-d07d-4c39-8e24-179324596f5f",
   "metadata": {},
   "source": [
    "## Pipelines time chunked; single-run catchup; backfillable\n",
    "Next blog post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "96b8c72a-3260-417c-bb3d-9b9a43c86a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The inputs are usually provided by the orchestrator\n",
    "\n",
    "# The logic depends on implementation; and some orchestrators provide backfill, but not catchup\n",
    "\n",
    "# start and end times\n",
    "\n",
    "# log table: catchup flag -> check log table for last end ts and run from them\n",
    "# Note: Airflow's catchup means it will run the pipeline from pipeline start date (one run per schedule till today)\n",
    "# backfill: run one time chunk at a time; favored over catchup for its ability to use prior chunks data for \n",
    "# current chunks computation\n",
    "\n",
    "# overwrites by partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60a4c3b-a590-4720-b384-b9d043e2a541",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
